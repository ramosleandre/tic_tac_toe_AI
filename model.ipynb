{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from conversion import CSVToTensor\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.input_size = 9\n",
    "        self.output_size = 9\n",
    "        self.hidden_size = 2*27\n",
    "\n",
    "        self.x0 = None\n",
    "        self.x1 = None\n",
    "        self.x2 = None\n",
    "        self.x3 = None\n",
    "\n",
    "        self.W1 = nn.Parameter(torch.randn(self.input_size, self.hidden_size))\n",
    "        self.b1 = nn.Parameter(torch.randn(self.hidden_size))\n",
    "\n",
    "        self.W2 = nn.Parameter(torch.randn(self.hidden_size, self.output_size))\n",
    "        self.b2 = nn.Parameter(torch.randn(self.output_size))\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.dropout = nn.Dropout(0.3)\n",
    "        self.crossloss = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.train_data = None\n",
    "        self.val_data = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x0 = x\n",
    "        self.x1 = x @ self.W1 + self.b1\n",
    "        self.x2 = self.relu(self.x1)\n",
    "        self.x3 = self.x2 @ self.W2 + self.b2\n",
    "        # x = self.dropout(x)\n",
    "        \n",
    "        return self.x3\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        dataloader = CSVToTensor(file_path)\n",
    "        dataloader.create_all_tensor()\n",
    "        dataset = dataloader.create_a_dataset()\n",
    "\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        self.train_data, self.val_data = random_split(dataset, [train_size, val_size])\n",
    "        self.train_data = DataLoader(self.train_data, batch_size=16, shuffle=True)\n",
    "        self.val_data = DataLoader(self.val_data, batch_size=16, shuffle=True)\n",
    "\n",
    "    def train_model(self, epochs):\n",
    "        self.to(self.device)\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for src, trg in self.train_data:\n",
    "                src = src.to(self.device)\n",
    "                trg = trg.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.forward(src)\n",
    "                loss = self.crossloss(output, trg.argmax(dim=1))\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            avg_loss = epoch_loss / len(self.train_data)\n",
    "            print(f\"Epoch: {epoch}, Loss: {avg_loss:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     model = Model()\n",
    "#     # choose the dataset file path\n",
    "#     model.load_data('./Datasets/tic_tac_toe_500_games.csv')\n",
    "#     # choose the number of epochs\n",
    "#     with torch.no_grad():\n",
    "#         model.W1.copy_(torch.zeros(model.input_size, model.hidden_size))\n",
    "#         model.b1.copy_(torch.ones(model.hidden_size))\n",
    "#         model.W2.copy_(torch.zeros(model.hidden_size, model.output_size))\n",
    "#         model.b2.copy_(torch.ones(model.output_size))\n",
    "#     model.train_model(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Weight Calculation by Human Reflection\n",
    "\n",
    "The following three cells demonstrate how a neural network's weights can be manually calculated to understand how updates affect the network. This process is aimed at providing educational insight into debugging neural networks.\n",
    "\n",
    "We consider 6 possible input scenarios and manually calculate the appropriate weights to ensure correctness for each scenario without causing errors in others.\n",
    "\n",
    "(Note: This is purely for educational purposes to understand debugging and does not alter the models themselves.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "-----------------\n",
      "tensor([[1., 1., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "-----------------\n",
      "tensor([[0., 1., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "-----------------\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [1., 0., 0.]])\n",
      "-----------------\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "-----------------\n",
      "tensor([[0., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]])\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "mytensor1 = torch.tensor([[1,0,1],[0,0,0],[0,0,0]], dtype=torch.float32)\n",
    "mytensor1 = mytensor1.reshape(1,9)\n",
    "outtensor1 = torch.tensor([[0,2,0,0,0,0,0,0,0]], dtype=torch.float32)\n",
    "\n",
    "print(mytensor1.view(3,3))\n",
    "print(\"-----------------\")\n",
    "\n",
    "mytensor2 = torch.tensor([[1,1,0],[0,0,0],[0,0,0]], dtype=torch.float32)\n",
    "mytensor2 = mytensor2.reshape(1,9)\n",
    "outtensor2 = torch.tensor([[0,0,2,0,0,0,0,0,0]], dtype=torch.float32)\n",
    "\n",
    "print(mytensor2.view(3,3))\n",
    "print(\"-----------------\")\n",
    "\n",
    "mytensor3 = torch.tensor([[0,1,1],[0,0,0],[0,0,0]], dtype=torch.float32)\n",
    "mytensor3 = mytensor3.reshape(1,9)\n",
    "outtensor3 = torch.tensor([[2,0,0,0,0,0,0,0,0]], dtype=torch.float32)\n",
    "\n",
    "print(mytensor3.view(3,3))\n",
    "print(\"-----------------\")\n",
    "\n",
    "mytensor4 = torch.tensor([[1,0,0],[0,0,0],[1,0,0]], dtype=torch.float32)\n",
    "mytensor4 = mytensor4.reshape(1,9)\n",
    "outtensor4 = torch.tensor([[0,0,0,2,0,0,0,0,0]], dtype=torch.float32)\n",
    "\n",
    "print(mytensor4.view(3,3))\n",
    "print(\"-----------------\")\n",
    "\n",
    "mytensor5 = torch.tensor([[1,0,0],[1,0,0],[0,0,0]], dtype=torch.float32)\n",
    "mytensor5 = mytensor5.reshape(1,9)\n",
    "outtensor5 = torch.tensor([[0,0,0,0,0,0,2,0,0]], dtype=torch.float32)\n",
    "\n",
    "print(mytensor5.view(3,3))\n",
    "print(\"-----------------\")\n",
    "\n",
    "mytensor6 = torch.tensor([[0,0,0],[1,0,0],[1,0,0]], dtype=torch.float32)\n",
    "mytensor6 = mytensor6.reshape(1,9)\n",
    "outtensor6 = torch.tensor([[2,0,0,0,0,0,0,0,0]], dtype=torch.float32)\n",
    "\n",
    "print(mytensor6.view(3,3))\n",
    "print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-2.,  2.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., -2.,  2., -2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 2.,  0., -2., -2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., -2.,  0., -2.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 2., -2.,  0.,  0.,  0.,  0., -2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    M.W1.copy_(torch.zeros(M.input_size, M.hidden_size))\n",
    "    M.b1.copy_(torch.zeros(M.hidden_size))\n",
    "    M.W2.copy_(torch.zeros(M.hidden_size, M.output_size))\n",
    "    M.b2.copy_(torch.zeros(M.output_size))\n",
    "\n",
    "    M.W1[0, 1] = 2\n",
    "    M.W1[0, 0] = -2\n",
    "    M.W1[0, 3] = 2\n",
    "\n",
    "    M.W1[1,1] = -2\n",
    "    M.W1[1, 2] = 2\n",
    "    M.W1[1, 3] = -2\n",
    "        \n",
    "    M.W1[2, 0] = 2\n",
    "    M.W1[2, 2] = -2\n",
    "    M.W1[2 , 3] = - 2\n",
    "    \n",
    "    M.W1[3, 6] = 2\n",
    "    M.W1[3 ,3] = -2\n",
    "    M.W1[3, 1] = -2\n",
    "    \n",
    "    M.W1[6, 1] = -2\n",
    "    M.W1[6, 6] = -2\n",
    "    M.W1[6, 0] = 2\n",
    "    M.W2[0:M.output_size , 0:M.output_size] = torch.eye(M.output_size)\n",
    "print(M.W1)\n",
    "# print(M.b1)\n",
    "# print(M.W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1., 0., 0., 1., 0., 0.]])\n",
      "----------------\n",
      "tensor([[2., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<AddBackward0>)\n",
      "----------------\n",
      "Excepted \n",
      "tensor([[0., 0., 2., 0., 0., 0., 0., 0., 0.]])\n",
      "----------------\n",
      "Output argmax :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = M.forward(mytensor6)\n",
    "x0 = M.x0\n",
    "x1 = M.x1\n",
    "x2 = M.x2\n",
    "x3 = M.x3\n",
    "\n",
    "print(x0)\n",
    "print(\"----------------\")\n",
    "print(output)\n",
    "print(\"----------------\")\n",
    "print(\"Excepted \")\n",
    "print(outtensor2)\n",
    "print(\"----------------\")\n",
    "print(\"Output argmax :\")\n",
    "output.argmax()\n",
    "# loss = mytensor2 - outtensor2\n",
    "# loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = lambda in0, out0: out0-in0\n",
    "# example of how work lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Weight Calculation by Machine Reflection\n",
    "\n",
    "The following three cells demonstrate how a neural network's weights can be calculated using machine reflection to understand how updates affect the network. This process is aimed at providing educational insight into debugging neural networks.\n",
    "\n",
    "For each line of code, we'll reflect on the machine's operations to ensure correctness. We consider 6 possible input scenarios and calculate the appropriate weights to ensure they are correct for each scenario without causing errors in others.\n",
    "\n",
    "The values are manually updated based on the results of the backpropagation provided by the model.\n",
    "\n",
    "The dataset for training will be composed only of the 6 tensor that you seen above and will be in the ./Datasets/debug.csv\n",
    "\n",
    "(Note: This is purely for educational purposes to understand debugging through machine reflection and does not alter the models themselves.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.enable_grad():\n",
    "    M.W1 = nn.Parameter(torch.randn(M.input_size, M.hidden_size))\n",
    "    M.b1 = nn.Parameter(torch.randn(M.hidden_size))\n",
    "    M.W2 = nn.Parameter(torch.randn(M.hidden_size, M.output_size))\n",
    "    M.b2 = nn.Parameter(torch.randn(M.output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.7607,  1.1830, -2.1755,  0.4035,  0.5289,  0.4068, -0.3963, -0.0067,\n",
      "          0.2504, -0.4641,  0.8439, -0.2991, -1.2224,  0.1305, -0.7202,  0.7318,\n",
      "         -1.2005, -0.9026, -0.3240, -0.3731,  1.8200,  2.4043, -0.3637, -2.6069,\n",
      "         -0.4025, -0.6294,  0.0035,  0.8499, -0.3010,  0.7200,  0.0547, -2.3964,\n",
      "         -0.8217, -0.2373, -1.8279, -1.0222,  0.3012, -1.9624,  0.6025,  0.5132,\n",
      "         -1.7385,  0.4129, -1.8338,  1.1736,  1.1272, -1.1030, -1.5456, -0.5622,\n",
      "          0.8727,  0.2063,  1.4553, -0.7751, -0.0583,  0.4135],\n",
      "        [-0.7431, -0.3773, -1.9917,  0.3992,  0.0781,  1.3508,  0.7905, -1.1979,\n",
      "          0.0972,  0.8137, -1.5300, -2.3547,  2.3226,  0.3028, -1.1419,  1.4530,\n",
      "          0.1525, -0.8001, -0.6464, -1.0195, -1.0396, -1.4570,  0.9839,  0.7677,\n",
      "         -0.2743,  0.6185,  0.8377, -0.0437, -0.2232,  0.0956,  0.9923,  0.7170,\n",
      "          0.4228,  0.7770, -1.0367, -0.2835, -1.2049, -0.7741, -1.4930, -0.6746,\n",
      "         -0.1737,  0.0921, -0.4355,  0.8034,  0.0660, -1.6547,  0.9378,  1.1341,\n",
      "         -1.3513,  0.4137,  0.0704,  0.7661,  2.0265, -0.3728],\n",
      "        [ 0.7255, -1.7056,  0.2271, -1.6117, -1.0371,  0.6185,  0.0987, -0.9371,\n",
      "         -1.9110, -0.3444,  0.7177,  0.1794, -0.9013, -0.8284, -0.3569,  0.6946,\n",
      "         -0.9138,  0.8210,  1.1434,  0.2187, -1.4274, -1.4554, -0.2795,  0.3051,\n",
      "         -1.8026, -0.4282,  0.1699,  0.6988, -2.0296, -0.5339, -0.6980, -1.4130,\n",
      "          0.7463, -0.0708,  0.3349, -0.1369,  0.3230, -0.5011, -1.2257,  1.3909,\n",
      "          0.4915, -0.8435,  0.3664, -0.1070,  1.1320,  0.5373,  1.1335, -0.6928,\n",
      "         -0.6878, -0.0407,  1.0952,  0.5957, -0.0934, -0.2426],\n",
      "        [ 1.2529,  0.4340,  0.1267, -0.8925,  0.3621, -0.6735,  0.2149,  1.2196,\n",
      "          0.9631,  0.3217, -2.4902, -0.8094,  3.0356,  0.5866,  0.1348,  0.4721,\n",
      "         -0.4504,  0.4138, -0.8353, -0.1668, -0.0249, -1.0844,  1.1718,  0.4733,\n",
      "          0.2672,  1.5657,  0.0290,  1.0013,  0.5001, -1.4738, -0.1828, -0.3352,\n",
      "          0.6508,  1.3071,  0.7309, -0.0876,  0.1778,  2.3537,  2.2397, -0.7614,\n",
      "          0.9213, -0.8483,  0.6289, -0.5678,  1.4345,  0.1476,  0.7205,  0.4375,\n",
      "          1.3009, -2.3778,  0.9730, -0.0292,  0.0758, -0.8274],\n",
      "        [ 0.1790, -0.4452,  0.8321,  0.3907,  0.0609,  0.2378,  0.5652, -0.9836,\n",
      "          1.0447,  0.9941,  1.3451, -0.8043, -0.2045,  0.4023,  0.4802, -0.1669,\n",
      "         -0.1043,  0.2974,  1.5774,  0.3776,  0.6239, -0.4457, -2.0535,  0.6343,\n",
      "         -0.1782, -0.6774,  0.5792, -0.4951, -0.2183,  0.6700,  0.3782,  1.2123,\n",
      "         -0.1716,  0.8865,  0.6735,  0.5998,  0.9521, -0.3467,  0.9125,  1.5751,\n",
      "          0.0454, -0.8995,  1.2800, -0.2874, -1.7674,  0.5273, -0.3048,  0.6888,\n",
      "         -1.7005, -1.4222, -0.3949,  1.2486,  0.9641, -0.0263],\n",
      "        [-1.1427,  1.4250, -0.6998,  0.6640,  0.3127, -0.1114, -1.3900,  0.8992,\n",
      "         -0.0791, -1.9999, -0.6924,  0.0728,  2.3156,  0.2658, -1.8969,  0.2005,\n",
      "         -0.2636,  0.9131, -0.3275, -0.2217, -0.5069, -0.6989,  1.6498,  1.2815,\n",
      "         -0.3572, -1.5993,  0.7643, -0.6854, -0.6814, -1.2128,  0.1880,  0.5896,\n",
      "          0.4302,  1.0312, -2.5618, -0.5449,  1.4852, -0.8562,  0.7712,  0.2067,\n",
      "          0.5366, -0.7019,  1.6654,  1.9452,  0.5966, -0.6439,  0.7134, -0.0845,\n",
      "          0.9171, -0.6520,  0.4754, -1.5914, -0.3083,  1.5309],\n",
      "        [ 0.7224, -0.1526, -0.3780,  0.2114, -0.6630,  0.2279,  0.1396,  1.0721,\n",
      "          0.4287,  0.7319, -0.8289,  1.3526, -0.2303,  0.5003,  0.1895, -0.0158,\n",
      "          0.1273,  0.7977, -0.3786,  1.3562,  0.7322,  0.3337, -0.5678, -1.3866,\n",
      "         -3.1776, -0.1033,  0.4572, -0.3912,  0.4801,  1.1067,  0.8903, -1.0682,\n",
      "          0.4347, -1.0188,  1.2372,  2.6703,  0.8055, -0.1231, -0.2063,  1.3229,\n",
      "          1.1326, -0.7586,  0.6409,  0.1368,  0.5053,  1.0418, -1.5377,  1.0856,\n",
      "          1.3260,  0.3064, -0.7693, -0.5863, -1.7538, -0.2180],\n",
      "        [-0.8701, -1.1268, -1.8103, -0.0537, -0.0263,  0.3751,  0.7335, -1.5414,\n",
      "          0.6209, -1.4508, -0.7929, -1.2496,  0.2033, -0.6003,  0.5265, -0.3052,\n",
      "          1.0930,  0.5364,  1.2999, -0.8087, -0.5549, -0.9108, -1.6854,  0.2674,\n",
      "         -0.4172, -1.8424,  0.0194,  1.1518, -0.3467, -0.3602, -1.5074,  0.0233,\n",
      "          0.7058, -0.5115,  0.2834,  0.2932,  0.3776,  0.4648, -0.1100,  0.2533,\n",
      "          0.2124, -0.4134,  0.1491, -0.0137,  0.1051,  0.5482,  0.4028, -1.1746,\n",
      "          0.6714,  0.1078, -0.2712,  3.3892, -0.4101,  2.0662],\n",
      "        [-0.4078,  1.7897,  0.5808, -0.2050, -1.5710, -1.7462,  0.1398, -0.6467,\n",
      "         -0.3868,  0.5227, -0.4477,  0.5046,  0.0096,  1.5397,  1.0370, -0.4085,\n",
      "         -1.4817, -1.2924,  0.1809,  0.9288, -0.5877,  0.0814, -0.2732, -0.8790,\n",
      "         -0.5426, -0.3277,  0.8268, -1.2596,  1.3531,  0.9899,  0.9326,  0.2666,\n",
      "         -0.8145,  0.3778,  0.8416, -1.8590,  0.0878, -1.2834,  0.5576, -2.1125,\n",
      "         -0.1944,  0.8518,  1.8780,  0.1070,  0.0097,  1.5816,  0.5066,  1.1558,\n",
      "          0.9677, -1.3068,  0.6567,  0.2384, -0.7659,  1.1876]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-6.5871e-01, -9.4008e-02, -1.1489e+00,  4.3950e-01,  4.6067e-01,\n",
      "        -1.6274e-03,  7.9433e-01, -9.2829e-01, -1.7096e-01,  2.8078e-01,\n",
      "         3.3947e-01,  5.5714e-01, -8.0318e-01,  2.2576e-01, -1.2042e+00,\n",
      "         1.2784e+00, -5.5732e-01, -2.3673e-01,  2.0019e+00,  2.5799e-01,\n",
      "        -4.6902e-01, -4.9441e-01, -2.4762e-01, -1.0254e+00, -9.2285e-01,\n",
      "         1.1589e+00, -1.1820e+00, -2.6121e-01, -1.4741e+00,  4.8143e-01,\n",
      "        -2.4239e-01,  1.2347e+00,  3.4431e-01, -6.5601e-01, -3.7338e-01,\n",
      "         1.6124e+00,  4.6033e-01, -9.5041e-01, -9.9572e-03,  3.9700e-01,\n",
      "        -2.2264e-01,  3.1629e-01, -4.9422e-01, -5.2475e-01, -3.2423e-01,\n",
      "         1.0313e+00,  1.2524e+00, -1.6996e-01,  1.4563e+00, -2.1768e+00,\n",
      "        -3.0180e-02, -1.1393e-01,  1.8835e+00, -2.3154e+00],\n",
      "       requires_grad=True)\n",
      "----------------\n",
      "Parameter containing:\n",
      "tensor([[ 9.8187e-01, -1.4712e-01, -1.1523e+00,  1.4500e+00, -4.6374e-01,\n",
      "         -2.5294e-03, -1.6934e+00, -6.2441e-01, -1.2540e+00],\n",
      "        [-1.7518e-01, -7.4512e-01, -6.5391e-01, -4.7016e-01,  2.7386e-01,\n",
      "         -5.7570e-01, -7.2829e-01, -6.5735e-01, -1.3665e+00],\n",
      "        [-1.7502e+00,  8.5524e-01, -3.4192e+00, -2.5265e-01,  1.0416e-01,\n",
      "          6.4248e-02, -2.9139e-01,  2.1837e+00,  4.3854e-01],\n",
      "        [ 8.9054e-01,  1.3040e+00, -1.1719e-01, -5.4471e-01,  1.6780e+00,\n",
      "          1.0219e+00, -4.3219e-01, -9.4181e-01,  1.0593e+00],\n",
      "        [ 1.0473e+00,  5.5280e-01, -5.8202e-01,  1.2788e+00, -4.1806e-01,\n",
      "          1.9948e+00,  7.1053e-01,  6.3117e-01,  4.5673e-01],\n",
      "        [ 8.3653e-01, -8.2776e-01, -2.9967e+00, -1.2998e+00, -4.4674e-02,\n",
      "          4.5391e-01,  1.6718e+00, -9.0909e-02,  1.6404e+00],\n",
      "        [ 6.0845e-02, -4.4279e-01, -6.2977e-01,  2.0708e+00, -9.9373e-01,\n",
      "         -3.4383e-01,  9.6310e-01,  1.4030e+00,  2.5012e-01],\n",
      "        [ 2.0210e+00,  9.0678e-01,  2.3045e-01,  1.2385e+00,  3.2459e-01,\n",
      "          1.0239e+00,  2.8516e-01,  3.0598e-01,  1.4193e+00],\n",
      "        [ 3.7099e-01, -1.0779e+00, -5.2563e-01,  4.2353e-01, -3.4826e-01,\n",
      "         -4.4159e-01,  7.9454e-01, -1.7934e+00,  1.9318e+00],\n",
      "        [-1.1459e+00, -6.2572e-01, -2.0871e-02, -6.1688e-01, -5.1360e-01,\n",
      "          2.1440e+00,  8.6832e-01, -1.0956e+00,  8.4722e-01],\n",
      "        [-2.3197e-01,  1.0464e+00, -1.8430e-01, -1.1876e+00,  1.8291e-02,\n",
      "         -1.4967e-01, -7.1425e-01,  4.6167e-01,  3.7329e-02],\n",
      "        [ 1.2092e+00, -6.8510e-02,  1.7799e+00, -5.4749e-01, -1.1312e-01,\n",
      "          8.4806e-01, -2.4957e-01,  2.8896e-01,  9.7682e-02],\n",
      "        [-5.8789e-01, -3.2151e-01,  4.3877e-02, -1.5986e+00, -9.1201e-01,\n",
      "          5.5803e-01, -1.4832e+00, -8.0028e-01,  7.1702e-01],\n",
      "        [-6.9529e-01, -1.4041e+00,  1.0930e+00,  5.2728e-01,  6.0236e-01,\n",
      "         -5.2358e-02, -6.0797e-01,  3.5679e-02,  2.2267e-01],\n",
      "        [-1.0109e+00, -8.2170e-02,  8.0655e-02,  8.5252e-01, -8.5871e-01,\n",
      "         -1.3596e+00, -1.8378e+00,  3.3239e-01,  9.3780e-01],\n",
      "        [ 1.4183e+00,  2.0752e+00, -1.4791e-01, -1.2506e+00,  5.3870e-01,\n",
      "          8.0700e-01, -9.1099e-01,  7.1790e-01, -5.2539e-01],\n",
      "        [ 8.7586e-01, -4.9533e-01, -8.3986e-01,  1.4502e+00, -5.8451e-01,\n",
      "         -5.0573e-02,  1.8630e+00,  1.1301e+00, -1.4411e-01],\n",
      "        [ 7.2665e-01,  6.8042e-01,  1.0409e+00, -8.2688e-01, -4.5490e-01,\n",
      "          7.8466e-01, -7.6064e-01,  1.1708e+00,  5.5140e-01],\n",
      "        [-3.4961e-01,  8.3851e-01,  1.0224e+00,  1.0653e+00,  5.8342e-01,\n",
      "          4.3230e-01,  2.1998e+00, -2.1842e-01, -4.5233e-01],\n",
      "        [ 1.9978e-01,  1.1049e+00, -1.5407e-01,  5.2207e-01, -1.3702e+00,\n",
      "          9.4970e-01,  4.0196e-01, -7.4304e-01,  3.8399e-01],\n",
      "        [-1.0215e+00,  8.4098e-01,  1.3429e+00,  7.1440e-01, -3.6336e-01,\n",
      "         -1.3003e+00, -3.7456e-01, -2.2305e-01, -2.9218e-01],\n",
      "        [-2.5590e-01,  3.4062e-01, -8.9817e-01, -6.4954e-01, -1.1115e+00,\n",
      "         -9.1962e-01,  5.7044e-02, -3.0322e-01, -3.6894e-01],\n",
      "        [-7.9466e-01, -9.8398e-01,  8.0976e-01,  8.2001e-02,  5.2314e-01,\n",
      "         -1.0889e+00, -4.5652e-01, -8.7485e-01,  1.8268e+00],\n",
      "        [ 9.2259e-01, -2.6180e-01, -1.3813e+00, -7.4315e-01, -6.2767e-01,\n",
      "          6.6351e-01,  9.2571e-01, -4.2116e-01, -5.0567e-02],\n",
      "        [ 9.5378e-01, -3.4882e-01,  8.0698e-01, -1.5469e-01,  8.3873e-01,\n",
      "         -7.0028e-01, -1.7139e-02, -1.0163e+00, -1.0421e+00],\n",
      "        [-2.8485e-02, -2.4695e-01, -3.2433e-01,  1.3389e+00, -1.2804e+00,\n",
      "          1.1464e+00,  2.5571e+00,  1.0204e+00,  5.4640e-01],\n",
      "        [-3.4143e-01,  1.6544e-03,  1.0576e-01,  1.2776e+00,  1.2155e+00,\n",
      "          5.4691e-01,  2.0193e+00, -6.4234e-01, -7.3595e-01],\n",
      "        [ 2.3448e-01, -9.1708e-01,  4.2466e-01,  2.0485e-01, -4.4261e-01,\n",
      "          1.4755e+00,  1.4708e+00, -1.1218e+00,  3.3132e-01],\n",
      "        [ 1.2334e+00,  3.5665e-01,  1.4567e-01,  2.3825e+00, -5.2130e-01,\n",
      "          5.3204e-01, -4.5255e-01,  1.8073e-01,  3.4108e-01],\n",
      "        [ 4.2237e-01, -2.5996e-01,  1.6320e-01, -1.3482e+00, -1.4716e+00,\n",
      "          1.9731e-02, -2.1572e+00, -9.9969e-01,  3.9699e-01],\n",
      "        [-7.0598e-01,  9.1838e-01,  6.4736e-02, -2.0430e+00,  5.3014e-01,\n",
      "          1.3106e+00, -9.8122e-01,  2.5558e-01,  2.6290e-01],\n",
      "        [ 4.2124e-01,  2.9136e-01,  2.2865e+00,  1.2056e+00, -7.5831e-02,\n",
      "         -1.5160e+00,  8.1348e-02,  2.2622e-01,  7.0934e-01],\n",
      "        [-1.1684e+00,  2.2399e+00, -1.0413e+00, -1.6919e+00, -1.7449e+00,\n",
      "         -8.3862e-01,  3.8765e-01, -8.3584e-01,  6.3260e-01],\n",
      "        [ 1.4215e+00, -1.6224e+00,  2.6696e-01, -1.7545e-01, -1.0921e+00,\n",
      "         -5.5042e-01,  7.5838e-01, -6.6751e-01, -1.6860e+00],\n",
      "        [ 1.7724e-01,  2.3905e-02, -1.7286e+00, -2.8778e-01, -1.7737e+00,\n",
      "          2.3022e+00, -2.0523e+00,  1.3079e+00, -7.2319e-01],\n",
      "        [ 5.5924e-02, -4.9673e-01,  3.7939e-02,  7.6052e-01,  1.0307e+00,\n",
      "          3.3688e-01,  5.7155e-01,  2.4439e-02, -5.0787e-01],\n",
      "        [ 4.1168e-01,  2.5827e-01,  3.7579e-02,  6.5670e-01,  9.3380e-01,\n",
      "          1.3814e+00,  1.7549e+00,  5.1561e-01,  5.7209e-01],\n",
      "        [-1.2669e+00,  8.2708e-01,  1.9046e-01, -8.2328e-01, -1.4553e-01,\n",
      "         -1.6378e+00,  8.3370e-01, -1.9630e+00, -1.9574e-01],\n",
      "        [ 1.7916e+00, -1.1351e+00,  5.3572e-01, -1.3750e+00,  5.5974e-01,\n",
      "         -1.5477e-01,  4.1570e+00, -1.8161e-01,  1.6644e+00],\n",
      "        [ 1.2778e+00,  1.0902e+00,  2.5271e-01,  1.3664e+00,  9.8990e-01,\n",
      "          1.4028e+00,  9.4362e-01,  2.0580e-01,  2.7188e-01],\n",
      "        [ 1.3285e+00,  5.7290e-01, -1.6649e+00,  4.5686e-01, -7.9286e-01,\n",
      "         -1.3161e+00, -8.0294e-01,  2.9469e-01,  1.9639e-01],\n",
      "        [ 8.0484e-01, -1.2757e-01,  1.5594e+00, -4.5765e-01, -6.5525e-02,\n",
      "          8.3193e-01,  1.0730e+00, -8.5188e-01,  1.2023e+00],\n",
      "        [ 7.6152e-01, -3.8505e-01, -2.4744e-01,  1.3489e+00, -8.9882e-01,\n",
      "         -1.3238e+00,  4.3402e-01, -2.5146e-01,  5.1775e-01],\n",
      "        [-9.4220e-01, -2.1459e-01, -1.1039e+00, -4.7904e-01,  5.6307e-01,\n",
      "         -2.9859e-01, -2.7471e-01,  6.2836e-01, -5.5672e-02],\n",
      "        [ 6.5072e-01,  2.0156e+00, -6.9883e-01,  1.1839e+00,  9.6337e-01,\n",
      "          1.4810e-01, -5.9102e-01, -8.1188e-01, -1.6676e-01],\n",
      "        [ 8.0782e-01, -1.1035e+00, -1.1043e+00, -1.6528e+00, -7.2880e-01,\n",
      "          1.5311e+00,  9.4524e-01, -2.8858e+00,  6.6959e-01],\n",
      "        [ 1.5232e+00, -4.1713e-02,  1.4049e+00, -5.1576e-01,  7.8042e-01,\n",
      "          7.3774e-01,  9.3059e-01,  9.4447e-01,  7.2576e-01],\n",
      "        [-1.6579e-02,  4.5297e-01,  6.0736e-01, -2.1441e+00,  3.7407e-01,\n",
      "         -1.0953e-01,  4.5751e-01,  6.5916e-01, -5.5192e-01],\n",
      "        [ 1.0122e+00,  1.0556e+00, -2.7310e-01,  3.0015e-01, -2.6897e-01,\n",
      "          5.6633e-01,  1.2006e+00, -1.5842e+00,  1.1715e+00],\n",
      "        [ 1.0769e-02, -3.1584e-01, -5.5430e-02, -1.5134e-01, -2.5108e-01,\n",
      "         -9.8572e-01,  7.9579e-01, -1.3827e+00, -5.1196e-01],\n",
      "        [ 1.1564e-01, -2.0261e-01, -4.3351e-01, -1.0210e+00,  9.5320e-02,\n",
      "          7.1790e-01,  1.1076e-01,  5.1316e-01,  4.4692e-01],\n",
      "        [-2.0634e-01, -7.5200e-01,  1.6603e+00, -8.8735e-01, -1.8906e+00,\n",
      "         -7.9291e-01,  2.0897e+00,  4.1648e-01, -1.9581e+00],\n",
      "        [ 2.3737e-01, -3.1214e-01, -7.5633e-01,  1.6535e+00, -8.0661e-01,\n",
      "         -9.3147e-01,  1.3335e+00, -1.7314e+00,  1.4490e+00],\n",
      "        [ 4.9229e-01, -6.4602e-01, -2.7925e-01, -9.4100e-01,  6.8705e-01,\n",
      "         -3.7539e-01, -7.9898e-01,  5.3076e-01, -1.0456e+00]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.7177,  1.2942, -1.6132, -0.2367, -1.5876, -0.0753, -1.4602, -0.9798,\n",
      "        -0.6718], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W1_backup = M.W1\n",
    "W2_backup = M.W2\n",
    "b1_backup = M.b1\n",
    "b2_backup = M.b2\n",
    "\n",
    "print(W1_backup)\n",
    "print(b1_backup)\n",
    "print(\"----------------\")\n",
    "print(W2_backup)\n",
    "print(b2_backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.W1 = W1_backup\n",
    "M.W2 = W2_backup\n",
    "M.b1 = b1_backup\n",
    "M.b2 = b2_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the data for training,\n",
    "# we don't split the data because we will evaluate manually\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "path = './Datasets/debug.csv'\n",
    "loader = CSVToTensor(path)\n",
    "loader.create_all_tensor()\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_tensor, output_tensor):\n",
    "        self.input_tensor = input_tensor\n",
    "        self.output_tensor = output_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_data = self.input_tensor[idx]\n",
    "        output_data = self.output_tensor[idx]\n",
    "        return input_data, output_data\n",
    "\n",
    "input_tensor = loader.game_tensor\n",
    "output_tensor = loader.prediction_tensor\n",
    "\n",
    "combined_dataset = CustomDataset(input_tensor, output_tensor)\n",
    "\n",
    "combined_dataloader = DataLoader(combined_dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src :  tensor([[1., 1., 0., 0., 0., 0., 0., 0., 0.]]) trg :  tensor([[0., 0., 2., 0., 0., 0., 0., 0., 0.]])\n",
      "-----------------\n",
      "src :  tensor([[0., 1., 1., 0., 0., 0., 0., 0., 0.]]) trg :  tensor([[2., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "-----------------\n",
      "src :  tensor([[0., 0., 0., 1., 0., 0., 1., 0., 0.]]) trg :  tensor([[2., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "-----------------\n",
      "src :  tensor([[1., 0., 1., 0., 0., 0., 0., 0., 0.]]) trg :  tensor([[0., 2., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "-----------------\n",
      "src :  tensor([[1., 0., 0., 0., 0., 0., 1., 0., 0.]]) trg :  tensor([[0., 0., 0., 2., 0., 0., 0., 0., 0.]])\n",
      "-----------------\n",
      "src :  tensor([[1., 0., 0., 1., 0., 0., 0., 0., 0.]]) trg :  tensor([[0., 0., 0., 0., 0., 0., 2., 0., 0.]])\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for src, trg in combined_dataloader:\n",
    "    print(\"src : \" , src, \"trg : \" , trg)\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range (36):\n",
    "    for src, trg in combined_dataloader:\n",
    "        M.optimizer.zero_grad()\n",
    "\n",
    "        output = M.forward(src)\n",
    "\n",
    "        # loss = torch.pow(output - trg, 2).sum().sqrt()\n",
    "        loss = M.crossloss(output, trg.argmax(dim=1))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        M.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss :\t 6.234299659729004\n",
      "input :\t tensor([[1., 0., 0., 0., 0., 0., 1., 0., 0.]])\n",
      "output : tensor([[  4.9632,  -8.2869,   1.2158,  -1.2174, -10.2340,  -0.3738, -13.9424,\n",
      "          -8.7958,   1.2641]], grad_fn=<AddBackward0>)\n",
      "target : tensor([[0., 0., 0., 2., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "W1 grad :\t tensor([[ 1.6862e+03,  0.0000e+00,  2.4608e+03, -4.5621e+02, -1.2904e+03,\n",
      "         -1.3063e+03,  6.5331e+02, -1.8821e+03,  5.8170e+02,  0.0000e+00,\n",
      "          8.5032e+02,  0.0000e+00, -2.8109e+03,  1.8126e+03,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -1.0589e+03,  0.0000e+00,  3.4909e+02,\n",
      "         -7.5309e+02, -2.7704e+02,  5.7016e+02,  2.6361e+01,  5.0517e+02,\n",
      "         -6.7303e+01,  0.0000e+00,  9.5539e+01,  4.0936e+02,  0.0000e+00,\n",
      "         -4.2361e+02, -3.1539e+02,  1.3330e+03,  2.4388e+03,  0.0000e+00,\n",
      "          0.0000e+00, -5.5776e+02, -1.6763e+03,  2.9357e+03,  1.6128e+02,\n",
      "          0.0000e+00,  3.7424e+02,  1.4474e+02,  1.4145e+02, -6.9596e+02,\n",
      "          3.1385e+02,  3.4012e+02,  0.0000e+00,  2.9146e+02,  6.0155e+02,\n",
      "          0.0000e+00, -1.7672e+02, -5.2346e+02,  1.7017e+03],\n",
      "        [ 7.3530e+02, -4.4513e+02,  1.0316e+03,  0.0000e+00, -4.6239e+02,\n",
      "         -5.1916e+02,  5.6620e+02,  0.0000e+00, -3.6976e+02,  0.0000e+00,\n",
      "          6.5364e+02,  2.4547e+02, -2.8423e+02,  1.3582e+02,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -9.7280e+02,  0.0000e+00,  4.8710e+02,\n",
      "         -1.8443e+02, -2.0179e+02,  1.4440e+02,  1.0858e+02,  2.5458e+01,\n",
      "         -4.3248e+02,  0.0000e+00, -1.4333e+02,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  1.3051e+02,  5.1687e+02, -3.3416e+02,  0.0000e+00,\n",
      "          0.0000e+00, -4.9382e+02,  0.0000e+00,  7.3578e+02,  4.5703e+02,\n",
      "         -5.2801e+02,  0.0000e+00,  0.0000e+00,  1.7709e+02, -4.2278e+02,\n",
      "         -2.5947e+02,  0.0000e+00,  0.0000e+00, -2.7904e+01,  0.0000e+00,\n",
      "          0.0000e+00,  2.9518e+02,  0.0000e+00, -1.8016e+02],\n",
      "        [ 6.5077e+01, -4.4513e+02,  7.9240e+02, -1.7028e+02, -3.4495e+02,\n",
      "          2.8981e+02,  6.0660e+02,  5.4743e+01, -9.5146e+02,  0.0000e+00,\n",
      "          9.4057e+02,  2.4547e+02,  8.6006e+02,  9.7997e+02,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -1.2124e+03,  0.0000e+00,  7.0032e+02,\n",
      "         -3.8300e+01,  0.0000e+00,  4.5898e+02,  0.0000e+00, -5.9102e+02,\n",
      "          7.2023e+01,  0.0000e+00, -3.5138e+02,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  1.1111e+03,  0.0000e+00,  5.5951e+02,  0.0000e+00,\n",
      "          0.0000e+00, -7.5776e+02, -6.9644e+01,  3.5112e+02,  9.0080e+01,\n",
      "         -5.2801e+02,  0.0000e+00,  7.0852e+01,  0.0000e+00, -9.0528e+02,\n",
      "         -3.3856e+02,  0.0000e+00,  0.0000e+00,  2.6355e+02,  0.0000e+00,\n",
      "          0.0000e+00,  4.7190e+02,  0.0000e+00, -1.7369e+02],\n",
      "        [ 2.6417e+02,  0.0000e+00,  8.0141e+02,  3.1300e+02,  0.0000e+00,\n",
      "         -4.2553e+02, -5.5790e+02, -1.1874e+03,  0.0000e+00,  0.0000e+00,\n",
      "          1.1144e+00,  1.0839e+01, -5.4095e+02,  7.5986e+02,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -5.9240e+02, -7.5247e+01, -2.4387e+00, -6.7427e+02,  4.4848e+02,\n",
      "         -7.2362e+00,  5.8021e+00,  9.1442e+02, -3.7663e+00,  0.0000e+00,\n",
      "         -4.3158e+02,  0.0000e+00,  5.5726e+02,  9.1361e+02,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -3.9598e+02,  8.9244e+02, -2.2256e+02,\n",
      "          0.0000e+00, -5.6677e+00,  4.4287e+02,  3.0349e+02, -1.3335e+02,\n",
      "          1.9243e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.8876e+02,\n",
      "          0.0000e+00,  0.0000e+00, -5.1591e+02,  6.3427e+02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-6.9893e+01,  0.0000e+00, -2.7349e+01, -5.7638e+02,  0.0000e+00,\n",
      "          8.8187e+01,  0.0000e+00, -7.2903e+02,  0.0000e+00,  0.0000e+00,\n",
      "          6.5467e+01,  1.0839e+01, -7.1051e+02,  1.6173e+02,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          2.2461e+01,  0.0000e+00,  2.5314e+02,  5.7098e+02,  5.9415e+02,\n",
      "          4.2996e+02,  5.8021e+00, -2.4293e+02,  4.0559e+02,  0.0000e+00,\n",
      "         -7.9713e+00,  0.0000e+00,  2.4683e+02,  6.0771e+02,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -1.1656e+03,  2.2032e+02,  0.0000e+00,\n",
      "          0.0000e+00,  3.6857e+02, -3.6230e+02, -3.3066e+02, -8.2756e+01,\n",
      "         -1.5516e+01,  3.4012e+02,  0.0000e+00,  0.0000e+00, -1.2785e+01,\n",
      "          0.0000e+00,  0.0000e+00,  7.5493e+00,  1.0260e+03],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "W1 grad max :\t tensor(2935.7004)\n"
     ]
    }
   ],
   "source": [
    "print (\"\\nloss :\\t\", loss.item())\n",
    "print (\"input :\\t\", M.x0)\n",
    "print (\"output :\", output)\n",
    "print (\"target :\", trg)\n",
    "print (\"\\nW1 grad :\\t\", M.W1.grad)\n",
    "print ('W1 grad max :\\t', M.W1.grad.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input  : tensor([1., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "target : tensor([0., 0., 0., 2., 0., 0., 0., 0., 0.])\n",
      "debug output : tensor([ 13.8007,  14.4762,  -3.2627,   0.8664,  -0.0930,  14.1155,   7.1295,\n",
      "        -13.2803,   5.6219], grad_fn=<AddBackward0>)\n",
      "debug loss :\t tensor(14.4014, grad_fn=<NllLossBackward0>)\n",
      "W1 grad :\t tensor([[-6.0175e+02, -3.3428e+02,  0.0000e+00,  1.4126e+03,  4.9800e+02,\n",
      "          3.0197e+03, -7.3280e+02, -1.6629e+01,  6.2729e+02,  7.4260e+02,\n",
      "          7.0546e+02,  6.0493e+02,  2.5775e+02, -1.1757e+03,  0.0000e+00,\n",
      "          1.2607e+03,  0.0000e+00,  0.0000e+00, -1.0675e+03,  1.1895e+02,\n",
      "         -1.3616e+03,  4.5454e+02,  3.2306e+02,  0.0000e+00,  0.0000e+00,\n",
      "         -7.1341e+01,  0.0000e+00,  1.4906e+01,  0.0000e+00,  7.8248e+02,\n",
      "          1.4336e+03,  0.0000e+00, -2.0557e+02,  8.9485e-01,  0.0000e+00,\n",
      "         -6.2536e+02,  2.3915e+01,  0.0000e+00,  6.1461e+02,  2.4828e+01,\n",
      "          0.0000e+00, -2.1159e+02,  0.0000e+00,  4.8831e+02,  1.8171e+02,\n",
      "          1.0505e+03, -2.2808e+02,  6.2285e+02,  1.0036e+03,  0.0000e+00,\n",
      "          1.0542e+03,  0.0000e+00,  1.3195e+01,  0.0000e+00],\n",
      "        [ 0.0000e+00, -2.8967e+02,  0.0000e+00,  5.7407e+02,  5.7637e+02,\n",
      "          2.7033e+03,  8.8731e+02,  0.0000e+00,  1.1145e+03,  1.3863e+03,\n",
      "          0.0000e+00,  0.0000e+00, -1.9569e+02, -4.9994e+02,  0.0000e+00,\n",
      "         -1.2563e+03,  0.0000e+00,  0.0000e+00,  5.9325e+02,  0.0000e+00,\n",
      "         -8.6691e+02,  2.7243e+02,  4.9425e+02,  1.5691e+00,  0.0000e+00,\n",
      "          1.7477e+03,  0.0000e+00,  5.9493e+02,  0.0000e+00, -1.2188e+03,\n",
      "         -7.1303e+01, -1.7164e+02,  7.8582e+02, -3.3489e+02,  0.0000e+00,\n",
      "          3.1191e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.9070e+01,\n",
      "         -1.0764e+03, -2.1159e+02,  0.0000e+00,  8.2433e+02, -3.1131e+02,\n",
      "          0.0000e+00, -6.1238e+02, -3.0249e+02,  7.1486e+02,  0.0000e+00,\n",
      "          4.2645e+02,  1.1595e+03,  1.5723e+03,  0.0000e+00],\n",
      "        [ 4.5405e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          5.2407e+02,  4.8477e+02,  0.0000e+00,  0.0000e+00,  1.0172e+03,\n",
      "         -8.1153e+01,  6.9119e+01, -4.5212e+02,  0.0000e+00,  0.0000e+00,\n",
      "         -1.2363e+03,  0.0000e+00,  0.0000e+00,  1.2347e+03, -4.4758e+01,\n",
      "          0.0000e+00, -4.6423e+01,  1.7076e+02,  1.5691e+00,  0.0000e+00,\n",
      "          1.3477e+03,  0.0000e+00,  7.2064e+02,  0.0000e+00, -1.2761e+03,\n",
      "         -1.3899e+02, -1.7164e+02,  5.8246e+02, -3.3489e+02,  0.0000e+00,\n",
      "          3.0150e+02,  2.9783e+01,  0.0000e+00,  0.0000e+00, -1.5636e+02,\n",
      "         -1.0764e+03,  0.0000e+00,  0.0000e+00,  3.0374e+02, -7.2561e+02,\n",
      "          1.3190e+02, -2.1507e+02,  2.3941e+02, -8.7675e+00,  0.0000e+00,\n",
      "          2.6727e+01,  1.1595e+03,  5.7473e+02,  0.0000e+00],\n",
      "        [-9.9107e+02, -2.3517e+02,  0.0000e+00,  0.0000e+00,  3.2687e+01,\n",
      "          0.0000e+00,  2.2470e+02, -6.9446e+02,  1.6144e+01,  1.1224e+03,\n",
      "          0.0000e+00, -5.1641e+02, -1.1074e+02,  1.2379e+02,  0.0000e+00,\n",
      "         -8.2667e+02,  0.0000e+00, -4.6211e+02,  9.1769e+02,  1.7480e+02,\n",
      "          1.6380e+02, -4.5613e-01,  6.7457e+01,  0.0000e+00,  0.0000e+00,\n",
      "          9.8813e+02,  0.0000e+00,  5.7229e+02,  0.0000e+00, -8.7521e+02,\n",
      "          2.1185e+02,  0.0000e+00,  5.4154e+02,  8.9485e-01, -3.9334e+02,\n",
      "          2.0280e+02,  5.6607e+02,  6.1144e+02,  4.5749e+02, -8.8432e+01,\n",
      "         -1.0658e+03,  0.0000e+00, -4.1139e+02, -9.4766e-01, -4.6635e+02,\n",
      "          1.4964e+02, -3.0369e+02,  1.3535e+02, -6.1724e+00,  0.0000e+00,\n",
      "          8.7795e+01,  0.0000e+00,  1.7479e+02,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.6459e+03, -2.8133e+02,  0.0000e+00,  8.3853e+02, -4.6631e+01,\n",
      "          6.3583e+02, -9.6632e+02, -7.1609e+02, -4.6991e+02,  1.5016e+03,\n",
      "          7.8661e+02,  1.9401e+01, -1.1207e+02, -5.5173e+02,  0.0000e+00,\n",
      "          5.6741e+02,  0.0000e+00, -4.6211e+02,  6.0446e+02,  3.3851e+02,\n",
      "         -3.2910e+02,  2.2899e+02,  6.7894e+01,  0.0000e+00,  0.0000e+00,\n",
      "          4.4026e+02,  0.0000e+00,  5.2380e+02,  0.0000e+00, -2.0314e+02,\n",
      "          1.5777e+03,  0.0000e+00,  5.4375e+02,  0.0000e+00, -3.9334e+02,\n",
      "         -2.3299e+02,  5.6409e+02,  6.1144e+02,  1.0790e+03, -1.5657e+02,\n",
      "         -1.0658e+03,  0.0000e+00, -4.1139e+02,  3.5339e+01, -5.0541e+02,\n",
      "          1.0687e+03, -3.0454e+02,  1.3001e+03,  2.9192e+02,  0.0000e+00,\n",
      "          6.8388e+02,  0.0000e+00, -8.4878e+02,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "\n",
    "src_i = combined_dataloader.dataset.input_tensor[i]\n",
    "trg_i = combined_dataloader.dataset.output_tensor[i]\n",
    "\n",
    "print (\"\\ninput  :\", src_i)\n",
    "print (\"target :\", trg_i)\n",
    "\n",
    "debug_output = M.forward(src_i)\n",
    "print (\"debug output :\", debug_output)\n",
    "\n",
    "# debug_loss = torch.pow(debug_output - trg_i, 2).sum().sqrt()\n",
    "debug_loss = M.crossloss(debug_output, trg_i.argmax(dim=0))\n",
    "print (\"debug loss :\\t\", debug_loss)\n",
    "\n",
    "debug_loss.backward()\n",
    "print (\"W1 grad :\\t\", M.W1.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST FOR UNDERSTAND HOW TO DEBUG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output : tensor([ 1.5546,  3.0224, -6.8387, -7.0046, -1.3878,  2.8336,  0.3625,  1.7013,\n",
      "        -6.9491], grad_fn=<AddBackward0>)\n",
      "prediction : tensor([0., 2., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor(-14.7057, grad_fn=<SumBackward0>)\n",
      "tensor([[ 10.6867,  -1.9877,   1.1917,  -0.5787, -22.9481,   0.0000, -15.2521,\n",
      "           0.5593,   0.0000,   0.0000,   0.0000,  26.1689, -19.8233,   0.0000,\n",
      "           0.2757,   0.0000,   7.1038,   6.2546,  -2.3206, -12.3942,   1.0800,\n",
      "         -18.5428,   0.7590,  -0.8824,   0.0000,   0.0000,   2.3690,  13.4138,\n",
      "         -19.3113,  -0.0856,  -0.7895,  21.0843,  -6.5768, -32.5610,  16.7636,\n",
      "          -2.5015,   2.3915,  -9.3138,  -8.5426,  11.7095,   0.0000,  -3.1289,\n",
      "           1.1421, -20.1334,  -0.4368,  -3.1831,   5.8103,  -7.9939,  30.7856,\n",
      "           1.6191,  -7.0927,   3.2435,   0.0000,   4.6845],\n",
      "        [  3.5622,  -0.4969,   0.4767,  -0.2894, -11.4740,   0.0000,  -7.6260,\n",
      "           0.0000,  -0.1904,   0.0000,   0.0000,  10.4675,  -7.9293,  -2.2635,\n",
      "           0.2757,   0.0000,   2.8415,   2.5018,  -4.6413,  -4.9577,   0.5400,\n",
      "          -9.2714,   0.3036,  -0.2941,   0.0000,   0.0000,   1.1845,   5.3655,\n",
      "          -4.8278,  -0.1712,   0.0000,   8.4337, -13.1536, -16.2805,   4.1909,\n",
      "           0.0000,   4.7830,  -6.2092,  -4.2713,   2.9274,   0.0000,  -6.2579,\n",
      "           0.0000,  -8.0534,  -0.1747,  -3.1831,   5.8103,  -3.1976,   6.1571,\n",
      "           0.0000,  -2.8371,   0.8109,   0.0000,   1.8738],\n",
      "        [ 10.6867,  -1.4908,   0.7150,  -0.4340, -17.2111,   0.0000, -11.4390,\n",
      "           0.0000,  -0.1904,   0.0000,   0.0000,  15.7013, -11.8940,  -2.2635,\n",
      "           0.2757,   0.0000,   4.2623,   3.7527,  -2.3206,  -7.4365,   0.8100,\n",
      "         -13.9071,   0.4554,   0.0000,   0.0000,   0.0000,   0.0000,   8.0483,\n",
      "         -14.4834,  -0.0856,   0.0000,  12.6506,  -6.5768, -24.4207,   8.3818,\n",
      "           0.0000,   2.3915,  -9.3138,   0.0000,   5.8547,   0.0000,  -3.1289,\n",
      "           0.0000, -12.0800,  -0.2621,   0.0000,   0.0000,  -4.7964,  12.3143,\n",
      "           0.0000,  -4.2556,   1.6218,   0.0000,   2.8107],\n",
      "        [  3.5622,  -0.9939,   0.4767,  -0.1447,  -5.7370,   0.0000,  -3.8130,\n",
      "           1.1185,   0.0000,   0.0000,   0.0000,  10.4675,  -3.9647,   0.0000,\n",
      "           0.2757,   0.0000,   2.8415,   1.2509,   0.0000,  -4.9577,   0.2700,\n",
      "           0.0000,   0.3036,  -0.5883,   0.0000,   0.0000,   0.0000,   5.3655,\n",
      "          -9.6556,  -0.0856,  -0.7895,   8.4337,   0.0000, -16.2805,   4.1909,\n",
      "          -5.0029,   0.0000,   0.0000,  -8.5426,   2.9274,   0.0000,   0.0000,\n",
      "           1.1421,  -8.0534,  -0.1747,   0.0000,   0.0000,  -1.5988,  12.3143,\n",
      "           1.6191,  -2.8371,   0.8109,   0.0000,   1.8738],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,  -0.9939,   0.4767,   0.0000, -11.4740,   0.0000,   0.0000,\n",
      "           0.5593,   0.0000,   0.0000,   0.0000,  10.4675,  -3.9647,   0.0000,\n",
      "           0.5513,   0.0000,   2.8415,   1.2509,   0.0000,  -4.9577,   0.5400,\n",
      "          -4.6357,   0.3036,  -0.5883,   0.0000,   0.0000,   1.1845,   5.3655,\n",
      "          -9.6556,  -0.0856,  -1.5789,   8.4337,   0.0000,  -8.1402,   0.0000,\n",
      "          -2.5015,   0.0000,   0.0000,  -4.2713,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,  -8.0534,  -0.1747,   0.0000,   0.0000,  -1.5988,  12.3143,\n",
      "           3.2381,  -2.8371,   0.0000,   0.0000,   1.8738],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000]])\n"
     ]
    }
   ],
   "source": [
    "with torch.enable_grad():\n",
    "    i = 0\n",
    "    \n",
    "    src = inputdata.dataset[0]\n",
    "\n",
    "    output = M.forward(src)\n",
    "    \n",
    "    print(\"output :\", output)\n",
    "\n",
    "    print(\"prediction :\", loader.prediction_tensor[i])\n",
    "\n",
    "    loss = (output - loader.prediction_tensor[i]).sum()\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    print(M.W1.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2.7487,  -8.7213,   1.5931,   1.6779,  -9.6748,  -4.3287, -11.8560,\n",
      "          -6.7841,   1.7666]])\n",
      "tensor(47.7955)\n",
      "tensor(19.7055)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    M.W1.grad = M.W1.grad * 0\n",
    "    newoutput = M.forward(src)\n",
    "    print(newoutput)\n",
    "    newloss = torch.abs(newoutput - loader.prediction_tensor[i]).sum()\n",
    "    print(newloss)\n",
    "    newloss = torch.pow(newoutput - loader.prediction_tensor[i], 2).sum().sqrt()\n",
    "    print(newloss)\n",
    "    # newloss.backward()\n",
    "    M.W1 -= (M.W1.grad) * 0.0001\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 2., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(loader.prediction_tensor[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque :\n",
    "- Si l'on commence un entrainement et que le modle ne devient pas assez prcis c'est fini.\n",
    "- Ma fonction de loss n'tait pas assez prcise ainsi les gradient devenait de plus en plus grand et donc l'ajustement divergeait\n",
    "- Lorsque que j'ai chang ma fonction de loss j'ai re-entrain mais sans amlioration (tant donn que les tenseurs taient dja bien trop grand c'tait impossible  rattraper)\n",
    "- lorsque j'ai reset les matrice est r-entraine j'ai obtenu une loss + que correcte (environ 0.0003)\n",
    "- Sur un dataset petit (donc surentrainement pour les valeurs actuelle) si le batch_size fait la taille du dataset l'entrainement n'est pas possible cela diverge trop\n",
    "- si il est de moiti l'entrainement  du mal tout de mme, il faut donc pour les gros dataset, trouver le rapport (efficience / rapidit)\n",
    "- lorsque l'on train donc sur un batch_size de 1 sans que la loss diverge (alatoire j'ai bien l'impression)\n",
    "- update : la loss qui tait petit  la fin de mon entrainement correspondait  un input unique ainsi les autres test n'taient pas si concluant.....\n",
    "problmatique :\n",
    "- il faut le faire tourner un nombre N* la taille du datasets ?\n",
    "- faut'il amliorer le batch_size afin d'avoir un cohrence d'erreur pour tous ?\n",
    "-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
